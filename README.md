# Using Image Segmentation in Determining the Readiness of Sweet Basil Seedlings for Plant Growing
This repositoru is for the results and codes used in assessing readiness for sweet basil seedlings.
The hardware used in this project is an Intel® Core™ i7-8750H CPU with 6 cores and 12 logical processors, with an NVIDIA GeForce GTX 1050 Ti GPU, and total available RAM of 16 GB and 8 GB shared GPU memory.

## Installation and Setup
To be able to run the code, the environment needs to be setup properly for Mask R-CNN. Please follow the instructions from https://www.immersivelimit.com/tutorials/mask-rcnn-for-windows-10-tensorflow-2-cuda-101 to install all the necessary pre-requisites.

## Running the codes

#### *Dataset*
* Images can be found [here](https://drive.google.com/drive/folders/1nga2TyWUemc9DHM6kIWc-S-rYOi73gq8?usp=sharing). These contain the images for train, validation, and test sets for the original resolution, splitted, and cross-validation fold images.
* cross-validation set images are folders labelled with **_cv_**
* all of the images are located in the **all** folder

Use these to skip steps 1 and 2
* Annotations can be found in the annotations folder. Each of the subfolders mentioned below should have a json file that has either **_train_** or **_val_** label which are used for train, and validation sets respectively.
  * For leaf and plant related annotations. go to folder **leaf and plant**. It should contain a folder names final_annotaitons where the following is found
    * leaf and plant
    * leaf only
    * plant only
  * For plant ready or not ready. go to folder **ready and not ready**. It should contain a folder named *final_annotations*

#### Step 1: Run the data_preloader_1.py
This file generates augmentations for the annotations and returns a json file type dictionary in COCO format annotation.

* define the following:
  * img_load_path: folder where the full resolution images are located. This foldername is _all_ in the images dataset link
  * annotate_path: the filepath to where the annotations generated by SuperAnnotate is located. Use the _annotations.json_ file from either the **leaf and plant** or **plant ready or not ready** folders
  * output_path: the filepath (including filename) you want the created json file to be saved to
  * filenames: the filename of densely populated images (e.g. IMG_20210519_203114.jpg)
  * class_id: the target class id stored in the selected _annotations.json_ file. The class ids are listed as follows:
    * [1]  : used when doing augmentations for _leaf class_ Only. This is used for the pre-training step
    * [5]  : used when doing augmentations for _plant class_ Only. This is used for the pre-training step
    * [1,5]: used when doing augmentations for the _leaf and plant_ class. This is input for method 1
    * [5,6]: used when doing augmentations for the _plant-ready and plant-not ready_ class. This is the input for method 2

#### Step 2: Run the data_preloader_2.py
This file generates the image augmentations and relates them with the augmented annoations in step 1. The output for this is used as for the `CocoLikeDataset` class function in test_driver.py

* define the following:
  * rescale_size: a factor (0-1) to downsize the image to. This will depend on your GPU. For my GPU I resized it to 0.15 but can be upto 0.25.
  * class_id: the class id that will be used for training.
    When only 1 object (plant only & leaf only) use class_id = [1]
    When 2 objects (leaf or plant & plant-ready or not ready) (class_id = [1,2]
  * filenames: the filename of densely populated images (e.g. IMG_20210519_203114.jpg). Use same set of filenames from data_preloader_1.py
  * DO Filepaths for Train set and Validation set
    * img_path: the folder which contains the train set OR val set
    * save_image_path: the folder to save the augmented images to
    * annotate_path: the filepath to where the annotations generated by data_preloader_1.py is located.
    * output_path: the filepath to save the created COCO format dictionary. This is used for the `CocoLikeDataset` class function in test_driver.py

#### Step 3: Run the test_driver.py
This file runs the training, generates the predicted masks, and calculates the metrics
* ROOT_DIR: the location of the mask rcnn pre-requisites. including the pre-trained COCO weights
* BasilConfig class: the parameters set for the training
* CocoLikeDataset class: the custom image and mask annotations loader derived from utils.py
* Prepare dataset
  * using CocoLikeDataset()
  * define the filepath to where annotations dictionary from Step 2 is saved
  * define the folder to where the augmented images from Step 2 is saved
  * DO for train and validation sets
* Instantiate the model for Training
* Load starting weights either from **coco**, **last** model in logs folder, or **specific** model.h5 file
* define augmentation strategy
* define the learning rate scheduler
* Start Training
  * Train Leaf Only, Plant Only, Ready+Not_Ready all using coco weights for 5 epochs using "heads" layer
  * Train Leaf Only, Plant Only Ready+Not_Ready all using coco weights for next 5 epochs using "3+" layer
  * After selecting best pre-trained weights. Use it to train the Leaf+Plant, and Ready+Not_Ready dataset every 5 epochs using "3+" layer
  * Do the CV strategy as explained in paper
* Inferencing:
  * inference_weight(model_path=None) is set either to None or a specific filepath to desired model
  * display_image: displays the predicted images. For Leaf+Plant creates a dictionary for the Leaf Counting algorithm then saved to a certain filepath
  * compute_batch_ap: calculates the mAP, avg Precision, avg Recall, and mask overlaps. Set iou_threshold to [0.5, 0.65, 0.75]
